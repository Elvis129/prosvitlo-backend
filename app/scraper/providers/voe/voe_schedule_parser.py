"""
VOE (–í—ñ–Ω–Ω–∏—Ü—è–æ–±–ª–µ–Ω–µ—Ä–≥–æ) Schedule Parser –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
–§–∞–π–ª: app/scraper/providers/voe/voe_schedule_parser.py

–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ VOE –≥—Ä–∞—Ñ—ñ–∫—ñ–≤:
- –ú–æ–∂—É—Ç—å –±—É—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ PDF (–ø–æ—Ç—Ä–µ–±—É—î –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó)
- –ê–±–æ —É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω—å (PNG/JPG)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ HOE
- –ü–æ—Ç—Ä—ñ–±–µ–Ω –æ–∫—Ä–µ–º–∏–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è

URL: https://www.voe.com.ua/informatsiya-pro-cherhy-hrafika-pohodynnykh-vidklyuchen-hpv-1
"""

import requests
from bs4 import BeautifulSoup
from datetime import date, timedelta
from typing import List, Dict, Optional
import logging
import hashlib

logger = logging.getLogger(__name__)

VOE_SCHEDULE_PAGE = "https://www.voe.com.ua/informatsiya-pro-cherhy-hrafika-pohodynnykh-vidklyuchen-hpv-1"


def fetch_voe_schedule_images() -> List[Dict]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≥—Ä–∞—Ñ—ñ–∫–∏ –ì–ü–í –∑ VOE
    
    –Ø–∫—â–æ VOE –ø—É–±–ª—ñ–∫—É—î PDF - –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    –Ø–∫—â–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–∞–ø—Ä—è–º—É
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
        –§–æ—Ä–º–∞—Ç –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–∏–π –¥–æ HOE: [
            {
                'date': date(2026, 1, 28),
                'image_url': 'https://...',
                'recognized_text': '',  # –î–ª—è VOE –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
                'content_hash': 'abc123...',
                'region': 'voe'
            }
        ]
    """
    try:
        logger.info("üîç [VOE] –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≥—Ä–∞—Ñ—ñ–∫–∏...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(VOE_SCHEDULE_PAGE, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        schedules = []
        
        # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫–∏
        # –í–∞—Ä—ñ–∞–Ω—Ç 1: –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        image_links = soup.find_all('a', href=lambda x: x and any(ext in x.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']))
        
        if image_links:
            logger.info(f"üì∑ [VOE] –ó–Ω–∞–π–¥–µ–Ω–æ {len(image_links)} –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
            schedules = _parse_voe_image_links(image_links)
        
        # –í–∞—Ä—ñ–∞–Ω—Ç 2: –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ PDF
        if not schedules:
            pdf_links = soup.find_all('a', href=lambda x: x and '.pdf' in x.lower())
            
            if pdf_links:
                logger.info(f"üìÑ [VOE] –ó–Ω–∞–π–¥–µ–Ω–æ {len(pdf_links)} PDF —Ñ–∞–π–ª—ñ–≤")
                schedules = _parse_voe_pdf_links(pdf_links)
        
        # –í–∞—Ä—ñ–∞–Ω—Ç 3: –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
        if not schedules:
            images = soup.find_all('img', src=lambda x: x and 'schedule' in x.lower() or '–≥—Ä–∞—Ñ—ñ–∫' in x.lower())
            
            if images:
                logger.info(f"üñºÔ∏è [VOE] –ó–Ω–∞–π–¥–µ–Ω–æ {len(images)} –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ")
                schedules = _parse_voe_inline_images(images)
        
        logger.info(f"‚úÖ [VOE] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(schedules)} –≥—Ä–∞—Ñ—ñ–∫—ñ–≤")
        return schedules
        
    except requests.RequestException as e:
        logger.error(f"‚ùå [VOE] –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤: {e}")
        return []
    except Exception as e:
        logger.error(f"‚ùå [VOE] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –≥—Ä–∞—Ñ—ñ–∫—ñ–≤: {e}")
        logger.exception("–î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è:")
        return []


def _parse_voe_image_links(links) -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
    
    Args:
        links: –°–ø–∏—Å–æ–∫ BeautifulSoup <a> –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
    """
    schedules = []
    today = date.today()
    
    for link in links:
        try:
            href = link.get('href')
            if not href:
                continue
            
            # –Ø–∫—â–æ –≤—ñ–¥–Ω–æ—Å–Ω–∏–π URL - –¥–æ–¥–∞—î–º–æ –¥–æ–º–µ–Ω
            if href.startswith('/'):
                href = 'https://www.voe.com.ua' + href
            elif not href.startswith('http'):
                href = 'https://www.voe.com.ua/' + href
            
            # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞—Ç—É –∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É –∞–±–æ —Ç–µ–∫—Å—Ç—É –ø–æ—Å–∏–ª–∞–Ω–Ω—è
            link_text = link.get_text(strip=True)
            schedule_date = _parse_voe_date_from_text(href, link_text)
            
            if not schedule_date:
                # –Ø–∫—â–æ –¥–∞—Ç—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—å–æ–≥–æ–¥–Ω—ñ/–∑–∞–≤—Ç—Ä–∞
                schedule_date = today
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —Ö–µ—à –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–º—ñ–Ω
            content_hash = hashlib.md5(href.encode()).hexdigest()
            
            schedule = {
                'date': schedule_date,
                'image_url': href,
                'recognized_text': '',  # VOE –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–µ–∫—Å—Ç–æ–≤—É —Ä–æ–∑—à–∏—Ñ—Ä–æ–≤–∫—É
                'content_hash': content_hash,
                'region': 'voe',
                'source': 'image'
            }
            schedules.append(schedule)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [VOE] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –ø–æ—Å–∏–ª–∞–Ω–Ω—è: {e}")
            continue
    
    return schedules


def _parse_voe_pdf_links(links) -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ PDF –≥—Ä–∞—Ñ—ñ–∫–∏
    
    –£–í–ê–ì–ê: PDF –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –≤ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è OCR
    –î–ª—è —Ü—å–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω pdf2image —Ç–∞ poppler
    
    Args:
        links: –°–ø–∏—Å–æ–∫ BeautifulSoup <a> –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ (PDF URLs)
    """
    schedules = []
    today = date.today()
    
    for link in links:
        try:
            href = link.get('href')
            if not href:
                continue
            
            if href.startswith('/'):
                href = 'https://www.voe.com.ua' + href
            elif not href.startswith('http'):
                href = 'https://www.voe.com.ua/' + href
            
            link_text = link.get_text(strip=True)
            schedule_date = _parse_voe_date_from_text(href, link_text)
            
            if not schedule_date:
                schedule_date = today
            
            content_hash = hashlib.md5(href.encode()).hexdigest()
            
            schedule = {
                'date': schedule_date,
                'image_url': href,
                'recognized_text': '',
                'content_hash': content_hash,
                'region': 'voe',
                'source': 'pdf',  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —â–æ —Ü–µ PDF
                'needs_conversion': True  # –ü–æ—Ç—Ä–µ–±—É—î –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó
            }
            schedules.append(schedule)
            
            logger.info(f"üìÑ [VOE] PDF –≥—Ä–∞—Ñ—ñ–∫: {link_text} ({schedule_date})")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [VOE] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É PDF: {e}")
            continue
    
    return schedules


def _parse_voe_inline_images(images) -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —è–∫—ñ –≤–∂–µ –ø—Ä–∏—Å—É—Ç–Ω—ñ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
    
    Args:
        images: –°–ø–∏—Å–æ–∫ BeautifulSoup <img> –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
    """
    schedules = []
    today = date.today()
    
    for img in images:
        try:
            src = img.get('src')
            if not src:
                continue
            
            if src.startswith('/'):
                src = 'https://www.voe.com.ua' + src
            elif not src.startswith('http'):
                src = 'https://www.voe.com.ua/' + src
            
            alt_text = img.get('alt', '')
            title_text = img.get('title', '')
            
            schedule_date = _parse_voe_date_from_text(src, f"{alt_text} {title_text}")
            
            if not schedule_date:
                schedule_date = today
            
            content_hash = hashlib.md5(src.encode()).hexdigest()
            
            schedule = {
                'date': schedule_date,
                'image_url': src,
                'recognized_text': '',
                'content_hash': content_hash,
                'region': 'voe',
                'source': 'inline_image'
            }
            schedules.append(schedule)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [VOE] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")
            continue
    
    return schedules


def _parse_voe_date_from_text(url: str, text: str) -> Optional[date]:
    """
    –í–∏—Ç—è–≥—É—î –¥–∞—Ç—É –∑ URL –∞–±–æ —Ç–µ–∫—Å—Ç—É
    
    –ú–æ–∂–ª–∏–≤—ñ —Ñ–æ—Ä–º–∞—Ç–∏:
    - "–ì–ü–í 15.01.2026"
    - "grafik-2026-01-15.jpg"
    - "–ì—Ä–∞—Ñ—ñ–∫ –Ω–∞ 15 —Å—ñ—á–Ω—è"
    - "–ó–∞–≤—Ç—Ä–∞" / "–°—å–æ–≥–æ–¥–Ω—ñ"
    
    Args:
        url: URL —Ñ–∞–π–ª—É
        text: –¢–µ–∫—Å—Ç –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∞–±–æ alt
    
    Returns:
        date –∞–±–æ None
    """
    import re
    
    combined_text = f"{url} {text}".lower()
    today = date.today()
    
    # –í–∞—Ä—ñ–∞–Ω—Ç 1: –î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç—ñ DD.MM.YYYY
    pattern1 = r'(\d{2})\.(\d{2})\.(\d{4})'
    match1 = re.search(pattern1, combined_text)
    
    if match1:
        day, month, year = map(int, match1.groups())
        try:
            return date(year, month, day)
        except ValueError:
            pass
    
    # –í–∞—Ä—ñ–∞–Ω—Ç 2: –î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç—ñ YYYY-MM-DD
    pattern2 = r'(\d{4})-(\d{2})-(\d{2})'
    match2 = re.search(pattern2, combined_text)
    
    if match2:
        year, month, day = map(int, match2.groups())
        try:
            return date(year, month, day)
        except ValueError:
            pass
    
    # –í–∞—Ä—ñ–∞–Ω—Ç 3: –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
    if '–∑–∞–≤—Ç—Ä–∞' in combined_text or 'tomorrow' in combined_text:
        return today + timedelta(days=1)
    
    if '—Å—å–æ–≥–æ–¥–Ω—ñ' in combined_text or 'today' in combined_text:
        return today
    
    if '–ø—ñ—Å–ª—è–∑–∞–≤—Ç—Ä–∞' in combined_text:
        return today + timedelta(days=2)
    
    # –í–∞—Ä—ñ–∞–Ω—Ç 4: –î–∞—Ç–∞ –∑ –º—ñ—Å—è—Ü–µ–º —Å–ª–æ–≤–æ–º "15 —Å—ñ—á–Ω—è 2026"
    months_ua = {
        '—Å—ñ—á': 1, '–ª—é—Ç': 2, '–±–µ—Ä–µ–∑': 3, '–∫–≤—ñ—Ç': 4, '—Ç—Ä–∞–≤': 5, '—á–µ—Ä–≤': 6,
        '–ª–∏–ø': 7, '—Å–µ—Ä–ø': 8, '–≤–µ—Ä–µ—Å–Ω': 9, '–∂–æ–≤—Ç–Ω': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥': 11, '–≥—Ä—É–¥': 12
    }
    
    for month_name, month_num in months_ua.items():
        if month_name in combined_text:
            # –®—É–∫–∞—î–º–æ —á–∏—Å–ª–æ –ø–µ—Ä–µ–¥ –º—ñ—Å—è—Ü–µ–º
            pattern = rf'(\d{{1,2}})\s+{month_name}'
            match = re.search(pattern, combined_text)
            if match:
                day = int(match.group(1))
                # –†—ñ–∫ - –ø–æ—Ç–æ—á–Ω–∏–π –∞–±–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π
                year = today.year
                try:
                    result_date = date(year, month_num, day)
                    # –Ø–∫—â–æ –¥–∞—Ç–∞ –≤ –º–∏–Ω—É–ª–æ–º—É - –¥–æ–¥–∞—î–º–æ —Ä—ñ–∫
                    if result_date < today:
                        result_date = date(year + 1, month_num, day)
                    return result_date
                except ValueError:
                    pass
    
    logger.debug(f"‚ö†Ô∏è [VOE] –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ –¥–∞—Ç—É –∑: '{combined_text[:100]}'")
    return None


def parse_voe_queue_schedule(local_path: str) -> Dict:
    """
    –ü–∞—Ä—Å–∏—Ç—å –≥—Ä–∞—Ñ—ñ–∫ VOE –∑ —Ñ–∞–π–ª—É
    
    VOE –≥—Ä–∞—Ñ—ñ–∫–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏:
    - PDF –∑ —Ç–µ–∫—Å—Ç–æ–º —á–µ—Ä–≥ (–ø–∞—Ä—Å–∏–º–æ —Ç–µ–∫—Å—Ç)
    - –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –∫–æ–ª—å–æ—Ä–æ–≤–∏–º –≥—Ä–∞—Ñ—ñ–∫–æ–º (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ color parser)
    
    Args:
        local_path: –õ–æ–∫–∞–ª—å–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –≥—Ä–∞—Ñ—ñ–∫–∞
    
    Returns:
        Dict: –ú–∞–ø–∞ —á–µ—Ä–≥ –Ω–∞ –†–ï–ú
        {
            "–†–ï–ú –í—ñ–Ω–Ω–∏—Ü—è-1": 1,
            "–†–ï–ú –í—ñ–Ω–Ω–∏—Ü—è-2": 2,
            ...
        }
    """
    try:
        logger.info(f"üé® [VOE] –ü–∞—Ä—Å–∏–º–æ –≥—Ä–∞—Ñ—ñ–∫: {local_path}")
        
        # –Ø–∫—â–æ —Ü–µ PDF - –ø–∞—Ä—Å–∏–º–æ —Ç–µ–∫—Å—Ç
        if local_path.lower().endswith('.pdf'):
            logger.info("üìÑ [VOE] –ì—Ä–∞—Ñ—ñ–∫ —É —Ñ–æ—Ä–º–∞—Ç—ñ PDF")
            
            from app.scraper.providers.voe.voe_pdf_parser import parse_voe_pdf_schedule
            
            queues = parse_voe_pdf_schedule(local_path)
            
            if queues:
                logger.info(f"‚úÖ [VOE] –†–æ–∑–ø–∞—Ä—Å–æ–≤–∞–Ω–æ {len(queues)} —á–µ—Ä–≥ –∑ PDF")
                
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–µ—Ä–≥–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –†–ï–ú ‚Üí —á–µ—Ä–≥–∞
                # VOE —Ñ–æ—Ä–º–∞—Ç: "1.1", "1.2", "2.1", "2.2", etc.
                # –ü–µ—Ä—à–∞ —Ü–∏—Ñ—Ä–∞ - –†–ï–ú, –¥—Ä—É–≥–∞ - —á–µ—Ä–≥–∞
                rem_map = {}
                for queue_str, streets in queues.items():
                    try:
                        parts = queue_str.split('.')
                        if len(parts) == 2:
                            rem_num = int(parts[0])
                            queue_num = int(parts[1])
                            
                            # –°—Ç–≤–æ—Ä—é—î–º–æ –†–ï–ú –Ω–∞–∑–≤—É
                            rem_name = f"VOE-{rem_num}"
                            rem_map[rem_name] = queue_num
                            
                    except ValueError:
                        continue
                
                return rem_map
            else:
                logger.warning("‚ö†Ô∏è [VOE] PDF –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ")
                return {}
        
        # –Ø–∫—â–æ —Ü–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ color parser
        elif any(local_path.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):
            logger.info("üñºÔ∏è [VOE] –ì—Ä–∞—Ñ—ñ–∫ —É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
            
            try:
                from app.scraper.schedule_color_parser import parse_schedule_from_image
                parsed_data = parse_schedule_from_image(local_path)
                
                if parsed_data:
                    logger.info(f"‚úÖ [VOE] –†–æ–∑–ø–∞—Ä—Å–æ–≤–∞–Ω–æ {len(parsed_data)} —á–µ—Ä–≥")
                else:
                    logger.warning("‚ö†Ô∏è [VOE] –ì—Ä–∞—Ñ—ñ–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ")
                
                return parsed_data
                
            except Exception as e:
                logger.error(f"‚ùå [VOE] –ü–æ–º–∏–ª–∫–∞ color parser: {e}")
                return {}
        
        else:
            logger.warning(f"‚ö†Ô∏è [VOE] –ù–µ–≤—ñ–¥–æ–º–∏–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É: {local_path}")
            return {}
        
    except Exception as e:
        logger.error(f"‚ùå [VOE] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –≥—Ä–∞—Ñ—ñ–∫–∞: {e}")
        logger.exception("–î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è:")
        return {}


# –î–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ HOE API
def fetch_schedule_images():
    """Alias –¥–ª—è fetch_voe_schedule_images"""
    return fetch_voe_schedule_images()


def parse_queue_schedule(schedule_data: dict):
    """Alias –¥–ª—è parse_voe_queue_schedule"""
    return parse_voe_queue_schedule(schedule_data)
