"""
–ü–æ—à—É–∫ —Å–ø–∏—Å–∫—É –∞–¥—Ä–µ—Å –Ω–∞ VOE —Å–∞–π—Ç—ñ
"""
import requests
from bs4 import BeautifulSoup

print("=" * 60)
print("üîç –ü–û–®–£–ö –°–ü–ò–°–ö–£ –ê–î–†–ï–° –ù–ê VOE")
print("=" * 60)

# –ú–æ–∂–ª–∏–≤—ñ URL –¥–µ –º–æ–∂–µ –±—É—Ç–∏ —Å–ø–∏—Å–æ–∫ –∞–¥—Ä–µ—Å
urls_to_check = [
    "https://www.voe.com.ua/",
    "https://www.voe.com.ua/for-customers/",
    "https://www.voe.com.ua/perelik-adr/",
    "https://www.voe.com.ua/for_customers/perelik-adr/",
    "https://www.voe.com.ua/addresses/",
    "https://www.voe.com.ua/spisok-adresov/",
    "https://www.voe.com.ua/rem/",
    "https://www.voe.com.ua/territory/",
]

session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
})

print("\nüìÑ –ü–µ—Ä–µ–≤—ñ—Ä—è—é –º–æ–∂–ª–∏–≤—ñ URL –∑—ñ —Å–ø–∏—Å–∫–∞–º–∏ –∞–¥—Ä–µ—Å...\n")

for url in urls_to_check:
    try:
        print(f"–ü–µ—Ä–µ–≤—ñ—Ä—è—é: {url}")
        response = session.get(url, timeout=10)
        
        if response.status_code == 200:
            print(f"  ‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ —ñ—Å–Ω—É—î!")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –®—É–∫–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
            text = response.text.lower()
            keywords = ['–∞–¥—Ä–µ—Å', '–≤—É–ª–∏—Ü', '–±—É–¥–∏–Ω–∫', '—Ä–µ–º', '—Ä–∞–π–æ–Ω', '–ø–µ—Ä–µ–ª—ñ–∫']
            found_keywords = [kw for kw in keywords if kw in text]
            
            if found_keywords:
                print(f"  üîç –ó–Ω–∞–π–¥–µ–Ω–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {', '.join(found_keywords)}")
                
                # –®—É–∫–∞—î–º–æ —Ç–∞–±–ª–∏—Ü—ñ
                tables = soup.find_all('table')
                if tables:
                    print(f"  üìä –¢–∞–±–ª–∏—Ü—å: {len(tables)}")
                
                # –®—É–∫–∞—î–º–æ —Å–ø–∏—Å–∫–∏
                lists = soup.find_all(['ul', 'ol'])
                if lists:
                    print(f"  üìã –°–ø–∏—Å–∫—ñ–≤: {len(lists)}")
                
                # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤
                print(f"  üìù –ü–æ—á–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É:")
                print(f"     {soup.get_text()[:200].strip()}")
            
        elif response.status_code == 404:
            print(f"  ‚ùå 404 - —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –Ω–µ —ñ—Å–Ω—É—î")
        else:
            print(f"  ‚ö†Ô∏è  –ö–æ–¥: {response.status_code}")
            
    except Exception as e:
        print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
    
    print()

# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–µ–Ω—é –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó –Ω–∞ –≥–æ–ª–æ–≤–Ω—ñ–π
print("\n" + "=" * 60)
print("üîç –ê–ù–ê–õ–Ü–ó –ú–ï–ù–Æ –ù–ê–í–Ü–ì–ê–¶–Ü–á")
print("=" * 60)

try:
    response = session.get("https://www.voe.com.ua/", timeout=10)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # –®—É–∫–∞—î–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
    links = soup.find_all('a', href=True)
    
    print(f"\nüìã –ó–Ω–∞–π–¥–µ–Ω–æ {len(links)} –ø–æ—Å–∏–ª–∞–Ω—å")
    print("\nüîç –ü–æ—Å–∏–ª–∞–Ω–Ω—è —â–æ –º—ñ—Å—Ç—è—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:\n")
    
    for link in links:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        
        keywords = ['–∞–¥—Ä–µ—Å', '—Ä–µ–º', '—Ä–∞–π–æ–Ω', '–ø–µ—Ä–µ–ª—ñ–∫', '—Ç–µ—Ä–∏—Ç', '—Å–ø–∏—Å–æ–∫', 'address', 'territory']
        if any(kw in href.lower() or kw in text.lower() for kw in keywords):
            print(f"  {text}: {href}")

except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É –º–µ–Ω—é: {e}")

print("\n" + "=" * 60)
