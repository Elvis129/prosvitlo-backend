"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –æ–≥–æ–ª–æ—à–µ–Ω—å –∑ —Å–∞–π—Ç—É hoe.com.ua
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Å–∏—Å—Ç–µ–º—É –±–∞–∑–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω—É –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∑–º—ñ–Ω
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import logging
import hashlib
from datetime import datetime
import json
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SCHEDULE_URL = "https://hoe.com.ua/page/pogodinni-vidkljuchennja"
NEWS_URL = "https://hoe.com.ua/post/novini-kompaniji"
TEMPLATE_FILE = "cache/schedule_page_template.json"


def fetch_announcements() -> List[Dict[str, str]]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –æ–≥–æ–ª–æ—à–µ–Ω—å –∑ —Å–∞–π—Ç—É - –ø–æ—Ä—ñ–≤–Ω—é—î –∑ –±–∞–∑–æ–≤–∏–º —à–∞–±–ª–æ–Ω–æ–º
    –í—ñ–¥–ø—Ä–∞–≤–ª—è—î push –¢–Ü–õ–¨–ö–ò —è–∫—â–æ —î –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –≤—ñ–¥ —à–∞–±–ª–æ–Ω—É
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ –∑ –ø–æ–ª—è–º–∏: title, body, content_hash, source
    """
    announcements = []
    
    # 1. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –Ω–∞ –∑–º—ñ–Ω–∏
    schedule_changes = _check_schedule_page_changes()
    if schedule_changes:
        announcements.extend(schedule_changes)
    
    # 2. –ü–∞—Ä—Å–∏–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ –æ–∫—Ä–µ–º–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    news_announcements = _fetch_news_page()
    announcements.extend(news_announcements)
    
    # 3. –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ —Ö–µ—à–µ–º
    unique_announcements = _remove_duplicates(announcements)
    
    logger.info(f"–í—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(unique_announcements)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å")
    return unique_announcements


def _check_schedule_page_changes() -> List[Dict[str, str]]:
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Å—Ç–æ—Ä—ñ–Ω–∫—É –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –Ω–∞ –∑–º—ñ–Ω–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –±–∞–∑–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω—É
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(SCHEDULE_URL, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        content_div = soup.find('div', class_='content-main')
        
        if not content_div:
            logger.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –±–ª–æ–∫ –∑ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º")
            return []
        
        # –í–∏—Ç—è–≥—É—î–º–æ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –î–û –ø–µ—Ä—à–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        first_image = content_div.find('img')
        current_content = []
        
        # –î–æ–¥–∞—î–º–æ 'li', 'ul', 'ol' –¥–ª—è –∑–∞—Ö–æ–ø–ª–µ–Ω–Ω—è –±—É–ª–ª–µ—Ç—ñ–≤ —Ç–∞ —Å–ø–∏—Å–∫—ñ–≤
        for element in content_div.find_all(['p', 'h3', 'h4', 'li', 'ul', 'ol']):
            if first_image and element.find('img'):
                break
            if first_image and element.sourceline and first_image.sourceline:
                if element.sourceline >= first_image.sourceline:
                    break
            
            # –î–ª—è —Å–ø–∏—Å–∫—ñ–≤ –≤–∏—Ç—è–≥—É—î–º–æ –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ li
            if element.name in ['ul', 'ol']:
                for li in element.find_all('li', recursive=False):
                    text = li.get_text(strip=True)
                    if text and len(text) > 10:
                        current_content.append(text)
            else:
                text = element.get_text(strip=True)
                if text and len(text) > 10:
                    current_content.append(text)
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ —Ö–µ—à –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        current_hash = hashlib.md5('\n'.join(current_content).encode()).hexdigest()
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤–∏–π —à–∞–±–ª–æ–Ω
        template_data = _load_template()
        
        if not template_data:
            # –ü–µ—Ä—à–∏–π –∑–∞–ø—É—Å–∫ - –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫ —à–∞–±–ª–æ–Ω
            _save_template(current_content, current_hash)
            logger.info("‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ –±–∞–∑–æ–≤–∏–π —à–∞–±–ª–æ–Ω —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
            return []
        
        template_hash = template_data.get('hash')
        template_content = template_data.get('content', [])
        
        # –Ø–∫—â–æ —Ö–µ—à –Ω–µ –∑–º—ñ–Ω–∏–≤—Å—è - –Ω—ñ—á–æ–≥–æ –Ω–æ–≤–æ–≥–æ –Ω–µ–º–∞—î
        if current_hash == template_hash:
            logger.info("‚úì –°—Ç–æ—Ä—ñ–Ω–∫–∞ –Ω–µ –∑–º—ñ–Ω–∏–ª–∞—Å—è –≤—ñ–¥–Ω–æ—Å–Ω–æ —à–∞–±–ª–æ–Ω—É")
            return []
        
        # –•–µ—à –∑–º—ñ–Ω–∏–≤—Å—è - —à—É–∫–∞—î–º–æ —â–æ —Å–∞–º–µ
        logger.info("‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –∑–º—ñ–Ω–∏ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤!")
        announcements = _analyze_changes(template_content, current_content)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —à–∞–±–ª–æ–Ω (—è–∫—â–æ –∑–º—ñ–Ω–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ñ)
        # –®–∞–±–ª–æ–Ω –ù–ï –æ–Ω–æ–≤–ª—é—î—Ç—å—Å—è —è–∫—â–æ —î –≤–∞–∂–ª–∏–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
        has_important = any(
            'UPD' in a.get('title', '') or 
            '–ó–±—ñ–ª—å—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in a.get('title', '') 
            for a in announcements
        )
        
        if not has_important and announcements:
            # –¶–µ –ø—Ä–æ—Å—Ç–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –±–µ–∑ –≤–∞–∂–ª–∏–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å
            _save_template(current_content, current_hash)
            logger.info("‚úì –û–Ω–æ–≤–ª–µ–Ω–æ –±–∞–∑–æ–≤–∏–π —à–∞–±–ª–æ–Ω")
        
        return announcements
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤: {e}")
        return []


def _analyze_changes(template_content: List[str], current_content: List[str]) -> List[Dict[str, str]]:
    """
    –ê–Ω–∞–ª—ñ–∑—É—î –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ —à–∞–±–ª–æ–Ω–æ–º —Ç–∞ –ø–æ—Ç–æ—á–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
    –°—Ç–≤–æ—Ä—é—î –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑ –ù–û–í–ò–• –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤ + –¥–æ–¥–∞—î –∑–≤'—è–∑—É—é—á—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    """
    announcements = []
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–æ–≤—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ (—î –≤ current, –Ω–µ–º–∞—î –≤ template)
    new_indices_set = set()
    for i, para in enumerate(current_content):
        if para not in template_content:
            new_indices_set.add(i)
    
    if not new_indices_set:
        logger.info("–ó–º—ñ–Ω–∏ –≤–∏—è–≤–ª–µ–Ω—ñ, –∞–ª–µ –Ω–æ–≤–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤ –Ω–µ–º–∞—î")
        return []
    
    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(new_indices_set)} –ù–û–í–ò–• –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≥—Ä—É–ø–∏ –æ–≥–æ–ª–æ—à–µ–Ω—å
    # –õ–æ–≥—ñ–∫–∞: —è–∫—â–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ –º—ñ—Å—Ç—è—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –ø—Ä–æ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è - –≥—Ä—É–ø—É—î–º–æ —ó—Ö —Ä–∞–∑–æ–º
    
    i = 0
    while i < len(current_content):
        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å—Ç–∞—Ä—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏, —è–∫—â–æ –≤–æ–Ω–∏ –Ω–µ –∑–≤'—è–∑—É—é—á—ñ
        if i not in new_indices_set:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ –∑–≤'—è–∑—É—é—á–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ (–í—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ:, —Ç–æ—â–æ)
            para = current_content[i]
            if not ('–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ' in para.lower() and len(para) < 50):
                i += 1
                continue
        
        # –ó–Ω–∞–π—à–ª–∏ –Ω–æ–≤–∏–π –∞–±–æ –∑–≤'—è–∑—É—é—á–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ - –ø–æ—á–∏–Ω–∞—î–º–æ –≥—Ä—É–ø—É
        current_announcement = []
        start_idx = i
        
        # –®—É–∫–∞—î–º–æ –ø–æ—á–∞—Ç–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏)
        para = current_content[i]
        is_announcement_start = (
            '–∑–±—ñ–ª—å—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in para.lower() or
            '–∑–º–µ–Ω—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in para.lower() or
            '—Ä–æ–∑–ø–æ—Ä—è–¥–∂–µ–Ω–Ω—è–º –Ω–µ–∫' in para.lower() or
            '—Ä–æ–∑–ø–æ—Ä—è–¥–∂–µ–Ω–Ω—è –Ω–µ–∫' in para.lower() or
            para.startswith('UPD') or
            para.startswith('–û–Ω–æ–≤–ª–µ–Ω–Ω—è')
        )
        
        if is_announcement_start:
            # –¶–µ –ø–æ—á–∞—Ç–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è - –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ –ø–æ–≤'—è–∑–∞–Ω—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏
            current_announcement.append(para)
            i += 1
            
            # –î–æ–¥–∞—î–º–æ –≤—Å—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ —â–æ —Å—Ç–æ—Å—É—é—Ç—å—Å—è —Ü—å–æ–≥–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
            while i < len(current_content):
                next_para = current_content[i]
                
                # –ó—É–ø–∏–Ω—è—î–º–æ—Å—å —è–∫—â–æ —Ü–µ –Ω–æ–≤–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
                is_next_announcement = (
                    '–∑–±—ñ–ª—å—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in next_para.lower() or
                    '–∑–º–µ–Ω—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in next_para.lower() or
                    ('—Ä–æ–∑–ø–æ—Ä—è–¥–∂–µ–Ω–Ω—è–º –Ω–µ–∫' in next_para.lower() and i in new_indices_set) or
                    next_para.startswith('UPD') or
                    next_para.startswith('–û–Ω–æ–≤–ª–µ–Ω–Ω—è')
                )
                
                if is_next_announcement:
                    break
                
                # –î–æ–¥–∞—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —è–∫—â–æ –≤—ñ–Ω:
                # 1. –ù–æ–≤–∏–π, –ê–ë–û
                # 2. –ó–≤'—è–∑—É—é—á–∏–π ("–í—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ:", –∫–æ—Ä–æ—Ç–∫–∏–π), –ê–ë–û  
                # 3. –ú—ñ—Å—Ç–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —á–µ—Ä–≥–∏/–ø—ñ–¥—á–µ—Ä–≥–∏
                should_include = (
                    i in new_indices_set or
                    ('–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ' in next_para.lower() and len(next_para) < 50) or
                    '–ø—ñ–¥—á–µ—Ä–≥' in next_para.lower() or
                    next_para.strip().startswith('‚Ä¢') or
                    next_para.strip().startswith('-')
                )
                
                if should_include:
                    current_announcement.append(next_para)
                    i += 1
                else:
                    # –î–æ—Å—è–≥–ª–∏ –∫—ñ–Ω—Ü—è –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
                    break
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
            if current_announcement:
                _save_announcement(current_announcement, announcements, 'schedule_page')
        else:
            # –¶–µ –Ω–µ –ø–æ—á–∞—Ç–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
            i += 1
    
    logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–æ {len(announcements)} –æ–≥–æ–ª–æ—à–µ–Ω—å –∑—ñ –∑–º—ñ–Ω")
    return announcements


def _fetch_news_page() -> List[Dict[str, str]]:
    """
    –ü–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–∏–Ω–∏ –∑ –æ–∫—Ä–µ–º–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–æ–≤–∏–Ω –∫–æ–º–ø–∞–Ω—ñ—ó.
    –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ –∫–æ–∂–Ω—É –Ω–æ–≤–∏–Ω—É —Ç–∞ –≤–∏—Ç—è–≥—É—î –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç.
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(NEWS_URL, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        announcements = []
        
        # –®—É–∫–∞—î–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –ø–æ—Å—Ç–∏/–Ω–æ–≤–∏–Ω–∏
        post_links = soup.find_all('a', href=lambda x: x and '/post/' in x and x != NEWS_URL)
        
        if not post_links:
            logger.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –Ω–æ–≤–∏–Ω–∏")
            return []
        
        logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(post_links)} –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –Ω–æ–≤–∏–Ω–∏")
        
        # –í–∏—Ç—è–≥—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ URL
        seen_urls = set()
        unique_links = []
        for link in post_links:
            url = link.get('href')
            if url and url not in seen_urls and url != NEWS_URL:
                seen_urls.add(url)
                if not url.startswith('http'):
                    url = 'https://hoe.com.ua' + url
                unique_links.append(url)
        
        # –ë–µ—Ä–µ–º–æ 3 –æ—Å—Ç–∞–Ω–Ω—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –Ω–æ–≤–∏–Ω–∏
        for news_url in unique_links[:3]:
            try:
                news_response = requests.get(news_url, headers=headers, timeout=30)
                news_response.raise_for_status()
                news_response.encoding = 'utf-8'
                
                news_soup = BeautifulSoup(news_response.text, 'html.parser')
                
                # –®—É–∫–∞—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–∏–Ω–∏
                content_div = news_soup.find('div', class_='content-main')
                if not content_div:
                    content_div = news_soup.find('div', class_='post-content')
                
                if not content_div:
                    logger.warning(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {news_url}")
                    continue
                
                # –í–∏—Ç—è–≥—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = content_div.find(['h1', 'h2'])
                title = title_elem.get_text(strip=True) if title_elem else '–ù–æ–≤–∏–Ω–∞'
                
                # –í–∏—Ç—è–≥—É—î–º–æ –≤—Å—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏
                paragraphs = []
                for p in content_div.find_all('p'):
                    text = p.get_text(strip=True)
                    if text and len(text) > 10:
                        paragraphs.append(text)
                
                if not paragraphs:
                    continue
                
                full_text = f"{title}\n\n" + '\n\n'.join(paragraphs)
                content_hash = hashlib.md5(full_text.encode()).hexdigest()
                
                announcements.append({
                    'title': 'üì∞ –ù–æ–≤–∏–Ω–∞ –≤—ñ–¥ –•–º–µ–ª—å–Ω–∏—Ü—å–∫–æ–±–ª–µ–Ω–µ—Ä–≥–æ',
                    'body': full_text[:500],
                    'full_body': full_text,
                    'content_hash': content_hash,
                    'source': 'news_page',
                    'url': news_url
                })
                
                logger.info(f"‚úì –°–ø–∞—Ä—Å–µ–Ω–æ –Ω–æ–≤–∏–Ω—É: {title[:50]}")
                
            except Exception as e:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É –Ω–æ–≤–∏–Ω–∏ {news_url}: {e}")
                continue
        
        logger.info(f"–ó—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–æ–≤–∏–Ω –∑–Ω–∞–π–¥–µ–Ω–æ {len(announcements)} –Ω–æ–≤–∏–Ω")
        return announcements
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–æ–≤–∏–Ω: {e}")
        return []


def _save_announcement(paragraphs: List[str], announcements: List[Dict], source: str):
    """–ó–±–µ—Ä—ñ–≥–∞—î –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑—ñ —Å–ø–∏—Å–∫—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤"""
    full_text = '\n\n'.join(paragraphs)
    content_hash = hashlib.md5(full_text.encode()).hexdigest()
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    first_line = paragraphs[0][:100]
    if 'UPD' in first_line or '–û–Ω–æ–≤–ª–µ–Ω–Ω—è' in first_line:
        title = 'üîÑ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞ –≤—ñ–¥–∫–ª—é—á–µ–Ω—å'
    elif '–ó–±—ñ–ª—å—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in first_line:
        title = '‚ö†Ô∏è –ó–±—ñ–ª—å—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É –æ–±–º–µ–∂–µ–Ω—å'
    elif '–ó–º–µ–Ω—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É' in first_line:
        title = '‚úÖ –ó–º–µ–Ω—à–µ–Ω–Ω—è –æ–±—Å—è–≥—É –æ–±–º–µ–∂–µ–Ω—å'
    elif '–ì—Ä–∞—Ñ—ñ–∫ –æ–Ω–æ–≤–ª–µ–Ω–æ' in first_line or '–ù–æ–≤–∏–π –≥—Ä–∞—Ñ—ñ–∫' in first_line:
        title = 'üìä –û–Ω–æ–≤–ª–µ–Ω–æ –≥—Ä–∞—Ñ—ñ–∫ –≤—ñ–¥–∫–ª—é—á–µ–Ω—å'
    else:
        title = 'üì¢ –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è'
    
    announcements.append({
        'title': title,
        'body': full_text[:500],
        'full_body': full_text,
        'content_hash': content_hash,
        'source': source
    })


def _remove_duplicates(announcements: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """–í–∏–¥–∞–ª—è—î –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ —Ö–µ—à–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
    seen_hashes = set()
    unique = []
    
    for announcement in announcements:
        if announcement['content_hash'] not in seen_hashes:
            seen_hashes.add(announcement['content_hash'])
            unique.append(announcement)
        else:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç: {announcement['title'][:50]}")
    
    return unique


def _load_template() -> Optional[Dict]:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –±–∞–∑–æ–≤–∏–π —à–∞–±–ª–æ–Ω —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
    try:
        if os.path.exists(TEMPLATE_FILE):
            with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ —à–∞–±–ª–æ–Ω—É: {e}")
    return None


def _save_template(content: List[str], content_hash: str):
    """–ó–±–µ—Ä—ñ–≥–∞—î –±–∞–∑–æ–≤–∏–π —à–∞–±–ª–æ–Ω —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
    try:
        os.makedirs(os.path.dirname(TEMPLATE_FILE), exist_ok=True)
        template_data = {
            'hash': content_hash,
            'content': content,
            'updated_at': datetime.now().isoformat()
        }
        with open(TEMPLATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(template_data, f, ensure_ascii=False, indent=2)
        logger.info("‚úì –®–∞–±–ª–æ–Ω –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —à–∞–±–ª–æ–Ω—É: {e}")


def check_schedule_availability() -> Optional[Dict[str, any]]:
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ –≥—Ä–∞—Ñ—ñ–∫–∏ –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
    
    Returns:
        Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –∞–±–æ None
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(SCHEDULE_URL, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –®—É–∫–∞—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
        not_available_text = [
            '–≥—Ä–∞—Ñ—ñ–∫ —â–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π',
            '–≥—Ä–∞—Ñ—ñ–∫ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π',
            '—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –≤—ñ–¥—Å—É—Ç–Ω—è',
            '–ø–æ–≥–æ–¥–∏–Ω–Ω—ñ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è'
        ]
        
        page_text = soup.get_text().lower()
        
        for text in not_available_text:
            if text in page_text:
                return {
                    'available': False,
                    'message': '–ü–æ–≥–æ–¥–∏–Ω–Ω—ñ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è'
                }
        
        # –Ø–∫—â–æ —î —Ç–∞–±–ª–∏—Ü—è –∑ –≥—Ä–∞—Ñ—ñ–∫–∞–º–∏ - –∑–Ω–∞—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ñ
        schedule_table = soup.find('table')
        if schedule_table:
            return {
                'available': True,
                'message': '–ì—Ä–∞—Ñ—ñ–∫–∏ –≤—ñ–¥–∫–ª—é—á–µ–Ω—å –¥–æ—Å—Ç—É–ø–Ω—ñ'
            }
        
        return {
            'available': False,
            'message': '–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≥—Ä–∞—Ñ—ñ–∫–∏ –≤—ñ–¥—Å—É—Ç–Ω—è'
        }
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤: {e}")
        return None
