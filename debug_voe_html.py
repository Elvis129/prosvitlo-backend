"""
Debug —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ VOE —Å–∞–π—Ç—É
–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: python debug_voe_html.py
"""

import requests
from bs4 import BeautifulSoup
import sys
from datetime import date

def analyze_voe_page(url, page_name):
    """–î–µ—Ç–∞–ª—å–Ω–æ –∞–Ω–∞–ª—ñ–∑—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É VOE —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
    print(f"\n{'='*80}")
    print(f"üîç –ê–ù–ê–õ–Ü–ó: {page_name}")
    print(f"URL: {url}")
    print(f"{'='*80}\n")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'uk-UA,uk;q=0.9,en;q=0.8',
        }
        
        today = date.today()
        data = {
            'Year': str(today.year),
            'Month': str(today.month),
        }
        
        print(f"üì§ POST –¥–∞–Ω—ñ: {data}")
        response = requests.post(url, data=data, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"\n‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}")
        print(f"üìè –†–æ–∑–º—ñ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª—ñ–≤\n")
        
        # –ê–Ω–∞–ª—ñ–∑ 1: –®—É–∫–∞—î–º–æ —Ñ–æ—Ä–º–∏
        print("üìã –§–û–†–ú–ò:")
        forms = soup.find_all('form')
        if forms:
            for i, form in enumerate(forms, 1):
                print(f"  Form {i}:")
                print(f"    Action: {form.get('action')}")
                print(f"    Method: {form.get('method')}")
                inputs = form.find_all(['input', 'select'])
                for inp in inputs:
                    print(f"    - {inp.name}: name={inp.get('name')}, type={inp.get('type')}")
        else:
            print("  ‚ùå –§–æ—Ä–º –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        # –ê–Ω–∞–ª—ñ–∑ 2: –®—É–∫–∞—î–º–æ —Ç–∞–±–ª–∏—Ü—ñ
        print("\nüìä –¢–ê–ë–õ–ò–¶–Ü:")
        tables = soup.find_all('table')
        if tables:
            for i, table in enumerate(tables, 1):
                print(f"  Table {i}:")
                print(f"    Class: {table.get('class')}")
                print(f"    ID: {table.get('id')}")
                rows = table.find_all('tr')
                print(f"    –†—è–¥–∫—ñ–≤: {len(rows)}")
                if rows:
                    # –ü–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ (–∑–∞–≥–æ–ª–æ–≤–æ–∫)
                    headers = rows[0].find_all(['th', 'td'])
                    if headers:
                        print(f"    –ó–∞–≥–æ–ª–æ–≤–∫–∏: {[h.get_text(strip=True) for h in headers]}")
                    # –î—Ä—É–≥–∏–π —Ä—è–¥–æ–∫ (–¥–∞–Ω—ñ)
                    if len(rows) > 1:
                        cells = rows[1].find_all(['td', 'th'])
                        print(f"    –ü—Ä–∏–∫–ª–∞–¥ –¥–∞–Ω–∏—Ö: {[c.get_text(strip=True)[:30] for c in cells]}")
        else:
            print("  ‚ùå –¢–∞–±–ª–∏—Ü—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        # –ê–Ω–∞–ª—ñ–∑ 3: –®—É–∫–∞—î–º–æ div-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏
        print("\nüì¶ DIV –ö–û–ù–¢–ï–ô–ù–ï–†–ò:")
        common_classes = [
            'outage', 'disconnect', '–≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è', 'item', 'row', 
            'card', 'block', 'content', 'data', 'info'
        ]
        
        for cls in common_classes:
            divs = soup.find_all('div', class_=lambda x: x and cls in str(x).lower())
            if divs:
                print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ {len(divs)} div –∑ –∫–ª–∞—Å–æ–º *{cls}*")
                if divs:
                    first_div = divs[0]
                    print(f"    –ü—Ä–∏–∫–ª–∞–¥ –∫–ª–∞—Å—ñ–≤: {first_div.get('class')}")
                    text = first_div.get_text(strip=True)[:100]
                    print(f"    –¢–µ–∫—Å—Ç: {text}...")
        
        # –ê–Ω–∞–ª—ñ–∑ 4: –®—É–∫–∞—î–º–æ article –µ–ª–µ–º–µ–Ω—Ç–∏
        print("\nüì∞ ARTICLE –ï–õ–ï–ú–ï–ù–¢–ò:")
        articles = soup.find_all('article')
        if articles:
            print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ {len(articles)} article –µ–ª–µ–º–µ–Ω—Ç—ñ–≤")
            for i, art in enumerate(articles[:3], 1):
                print(f"  Article {i}: class={art.get('class')}")
        else:
            print("  ‚ùå Article –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        # –ê–Ω–∞–ª—ñ–∑ 5: –®—É–∫–∞—î–º–æ —Å–ø–∏—Å–∫–∏ (ul/ol)
        print("\nüìù –°–ü–ò–°–ö–ò:")
        lists = soup.find_all(['ul', 'ol'])
        if lists:
            print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ {len(lists)} —Å–ø–∏—Å–∫—ñ–≤")
            for i, lst in enumerate(lists[:3], 1):
                items = lst.find_all('li')
                print(f"  List {i}: {len(items)} –µ–ª–µ–º–µ–Ω—Ç—ñ–≤, class={lst.get('class')}")
        else:
            print("  ‚ùå –°–ø–∏—Å–∫—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        # –ê–Ω–∞–ª—ñ–∑ 6: –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        print("\nüèóÔ∏è –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–†–£–ö–¢–£–†–ê:")
        main_content = soup.find(['main', 'div'], id=lambda x: x and 'content' in str(x).lower())
        if main_content:
            print(f"  Main content: {main_content.name}, id={main_content.get('id')}")
        
        # –®—É–∫–∞—î–º–æ –±—É–¥—å-—è–∫—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –∑ —Ç–µ–∫—Å—Ç–æ–º —â–æ –º—ñ—Å—Ç–∏—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        print("\nüîç –ü–û–®–£–ö –ö–õ–Æ–ß–û–í–ò–• –°–õ–Ü–í:")
        keywords = ['–†–ï–ú', '–≤—É–ª.', '–≤—É–ª–∏—Ü—è', '–±—É–¥–∏–Ω–æ–∫', '—á–∞—Å', '–¥–∞—Ç–∞']
        for keyword in keywords:
            elements = soup.find_all(text=lambda x: x and keyword.lower() in str(x).lower())
            if elements:
                print(f"  '{keyword}': –∑–Ω–∞–π–¥–µ–Ω–æ {len(elements)} –∑–≥–∞–¥–æ–∫")
                # –ü–æ–∫–∞–∑—É—î–º–æ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
                parent_tags = set()
                for elem in elements[:5]:
                    if elem.parent:
                        parent_tags.add(elem.parent.name)
                print(f"    –ë–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ —Ç–µ–≥–∏: {parent_tags}")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        filename = f"debug_{page_name.lower().replace(' ', '_')}.html"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"\nüíæ HTML –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {filename}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """–ê–Ω–∞–ª—ñ–∑—É—î –≤—Å—ñ VOE —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
    print("\n" + "="*80)
    print("üöÄ VOE HTML STRUCTURE ANALYZER")
    print("="*80)
    
    pages = [
        ("https://www.voe.com.ua/disconnection/emergency", "Emergency Outages"),
        ("https://www.voe.com.ua/disconnection/planned", "Planned Outages"),
    ]
    
    results = {}
    
    for url, name in pages:
        results[name] = analyze_voe_page(url, name)
    
    print("\n" + "="*80)
    print("üìä –ü–Ü–î–°–£–ú–û–ö")
    print("="*80)
    
    for name, success in results.items():
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} {name}")
    
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á:")
    print("1. –ü–æ–¥–∏–≤—ñ—Ç—å—Å—è –∑–±–µ—Ä–µ–∂–µ–Ω—ñ HTML —Ñ–∞–π–ª–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É")
    print("2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ browser DevTools –¥–ª—è —ñ–Ω—Å–ø–µ–∫—Ü—ñ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
    print("3. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –∞–±–æ cookies")
    print("="*80)


if __name__ == "__main__":
    sys.exit(main())
